{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook to create my sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import glob2\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'samples\\\\mixer.wav'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spatial_two_mics.labels_inference.tf_label_estimator import TFMaskEstimator\n",
    "from spatial_two_mics.utils.audio_mixture_constructor import AudioMixtureConstructor\n",
    "\n",
    "ground_estimator = TFMaskEstimator('duet_kmeans')\n",
    "duet_estimator = TFMaskEstimator('ground_truth')\n",
    "\n",
    "audio_mixer = AudioMixtureConstructor()\n",
    "\n",
    "def transform_sample(sample_path, tf_sample_path, target_sr=16000):\n",
    "    \"\"\"Takes a sample that is not ready for use in the model, \n",
    "        and transforms it to a sample that the model can use.\n",
    "        @param sample_path - the path to the sample, a String\n",
    "        @param tf_sample_path - the path for saving the proccessed sample\n",
    "        @returns: a path to the transformed sample\"\"\"\n",
    "    signal, _ = librosa.core.load(sample_path, sr=target_sr, dtype=np.float32)\n",
    "    if len(signal.shape) > 1:\n",
    "        signal = signal[:,0]    \n",
    "    scipy.io.wavfile.write(tf_sample_path, target_sr, signal)\n",
    "    return tf_sample_path\n",
    "\n",
    "# a sound to mix with all of the other sounds\n",
    "mixer_wav = os.path.join('samples', 'mixer.wav')\n",
    "orig_wav = os.path.join('samples', '4kHz_sin_wave.wav')\n",
    "transform_sample(orig_wav, mixer_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(file_path):\n",
    "    return ({'positions':\n",
    "                      {'amplitudes': np.array([0.73382382, 0.26617618]),\n",
    "                       'd_thetas': np.array([1.06829948]),\n",
    "                       'distances': {'m1m1': 0.0,\n",
    "                                     'm1m2': 0.03,\n",
    "                                     'm1s1': 3.015,\n",
    "                                     'm1s2': 3.0072529608785676,\n",
    "                                     'm2m1': 0.03,\n",
    "                                     'm2m2': 0.0,\n",
    "                                     'm2s1': 2.985,\n",
    "                                     'm2s2': 2.9928046426867034,\n",
    "                                     's1m1': 3.015,\n",
    "                                     's1m2': 2.985,\n",
    "                                     's1s1': 0.0,\n",
    "                                     's1s2': 3.054656422155759,\n",
    "                                     's2m1': 3.0072529608785676,\n",
    "                                     's2m2': 2.9928046426867034,\n",
    "                                     's2s1': 3.054656422155759,\n",
    "                                     's2s2': 0.0},\n",
    "                       'taus': np.array([1.39941691, 0.67397403]),\n",
    "                       'thetas': np.array([0., 1.06829948]),\n",
    "                       'xy_positons': np.array([[3., 0.],\n",
    "                                             [1.44484569, 2.62914833]])},\n",
    "         'sources_ids': [{'gender': 'f',\n",
    "                         'sentence_id': 'sa1',\n",
    "                         'speaker_id': 'flbw0',\n",
    "                         'wav_path': file_path},\n",
    "                        {'gender': 'm',\n",
    "                         'sentence_id': 'sa2',\n",
    "                         'speaker_id': 'mbns0',\n",
    "                         'wav_path': mixer_wav}\n",
    "    ]})\n",
    "\n",
    "def get_mixture_info(file_path):\n",
    "    file_path = transform_sample(file_path, file_path.split('.')[0] + 'X.wav')\n",
    "    ex = get_info(file_path)\n",
    "    return audio_mixer.construct_mixture(mixture_info=ex)\n",
    "\n",
    "def get_masks(file_path):\n",
    "    T = get_mixture_info(file_path)\n",
    "    ground_mask = ground_estimator.infer_mixture_labels(T)\n",
    "    duet_mask = duet_estimator.infer_mixture_labels(T)\n",
    "    return ground_mask, duet_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sample_tf(sample_path, tf_path, target_sr=16000):\n",
    "    \"\"\"Takes a sample that is not ready for use in the model, \n",
    "        and transforms it to a sample that the model can use.\n",
    "        @param sample_path - the path to the sample, a String\n",
    "        @param tf_sample_path - the directory to save the information of the sample\n",
    "        @returns: a path to the transformed sample\"\"\"\n",
    "    signal, _ = librosa.core.load(sample_path, sr=target_sr, dtype=np.float32)\n",
    "    # take only the first channel\n",
    "    if len(signal.shape) > 1:\n",
    "        signal = signal[:,0]\n",
    "    # calculate the real part of the stft and the complex part of the stft\n",
    "    tf = librosa.core.stft(signal)\n",
    "    real = np.real(tf)\n",
    "    imag = np.imag(tf)\n",
    "    # write it to the file, seperate it to real and imag parts\n",
    "    real_p = os.path.join(tf_path, 'real_tfs')\n",
    "    imag_p = os.path.join(tf_path, 'imag_tfs')\n",
    "    joblib.dump(real, real_p, compress=0)\n",
    "    joblib.dump(imag, imag_p, compress=0)\n",
    "    # another thing for this to work:\n",
    "    # save the raw wavs in the name 'wavs'\n",
    "    wavs_p = os.path.join(tf_path, 'wavs')\n",
    "    joblib.dump(signal, wavs_p, compress=0)\n",
    "    # save the 'soft_labeled_mask' and 'ground_truth_mask'\n",
    "    ground_masks, duet_mask = get_masks(sample_path)\n",
    "    soft_p = os.path.join(tf_path, 'soft_labeled_mask')\n",
    "    ground_p = os.path.join(tf_path, 'ground_truth_mask')\n",
    "    joblib.dump(ground_masks, ground_p, compress=0)\n",
    "    joblib.dump(duet_mask, soft_p, compress=0)\n",
    "    return tf_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_path = \"samples\"\n",
    "target_path = os.path.join('tf_samples', 'my_dataset', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sep():\n",
    "    sep = '/'\n",
    "    windows = True\n",
    "    if windows:\n",
    "        sep = '\\\\'\n",
    "    return sep\n",
    "\n",
    "def extract_name(path):\n",
    "    # a file with the path - './.../folder/<name>.wav', returs <name>\n",
    "    file_name = path.split(sep())[-1]\n",
    "    return file_name.split('.')[0]\n",
    "\n",
    "# test:\n",
    "extract_name(\".\\\\some folder\\\\example.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['samples\\\\4kHz_sin_wave.wav',\n",
       " 'samples\\\\bass1.wav',\n",
       " 'samples\\\\bass2.wav',\n",
       " 'samples\\\\drums1.wav',\n",
       " 'samples\\\\drums2.wav',\n",
       " 'samples\\\\mixer.wav',\n",
       " 'samples\\\\vocals1.wav']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_list = glob2.glob(unprocessed_path + f'{sep()}*')\n",
    "sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_path in sample_list:\n",
    "    name = extract_name(sample_path)\n",
    "    tf_path = os.path.join(target_path, name)\n",
    "    if not os.path.isdir(tf_path):\n",
    "        os.mkdir(tf_path)\n",
    "    transform_sample_tf(sample_path, tf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also for *val* and for *test* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = os.path.join('tf_samples', 'my_dataset', 'val')\n",
    "sample_list = glob2.glob(unprocessed_path + f'{sep()}*')\n",
    "for sample_path in sample_list:\n",
    "    name = extract_name(sample_path)\n",
    "    tf_path = os.path.join(target_path, name)\n",
    "    if not os.path.isdir(tf_path):\n",
    "        os.mkdir(tf_path)\n",
    "    transform_sample_tf(sample_path, tf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = os.path.join('tf_samples', 'my_dataset', 'test')\n",
    "sample_list = glob2.glob(unprocessed_path + f'{sep()}*')\n",
    "for sample_path in sample_list:\n",
    "    name = extract_name(sample_path)\n",
    "    tf_path = os.path.join(target_path, name)\n",
    "    if not os.path.isdir(tf_path):\n",
    "        os.mkdir(tf_path)\n",
    "    transform_sample_tf(sample_path, tf_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-project",
   "language": "python",
   "name": "dl-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
